{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we will extend some of the TD algorithms we implemented in the previous tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook setup\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Import numpy, scipy and matplotlib\n",
    "- Configure inline plots\n",
    "- Import helper modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "# Import definitions of the environments.\n",
    "import RL_worlds as worlds\n",
    "\n",
    "# Import helper functions for plotting.\n",
    "from plot_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_state(params):\n",
    "    \"\"\"\n",
    "    Initialize the state at the beginning of an episode.\n",
    "    Args:\n",
    "        params: a dictionary containing the default parameters.\n",
    "    Returns:\n",
    "        an integer corresponding to the initial state.\n",
    "    \"\"\"\n",
    "    if params['environment'].name == 'windy_cliff_grid':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'n_armed_bandit':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'cheese_world':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'cliff_world':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'quentins_world':\n",
    "        return 54\n",
    "\n",
    "def update_state(state, action, params):\n",
    "    \"\"\"\n",
    "    State transition based on world, action and current state.\n",
    "    Args:\n",
    "        state: integer corresponding to the current state.\n",
    "        action: integer corresponding to the action taken.\n",
    "        params: a dictionary containing the default parameters.\n",
    "    Returns:\n",
    "        an integer corresponding to the next state;\n",
    "        an integer corresponding to the reward received.\n",
    "    \"\"\"\n",
    "    next_state, reward = params['environment'].get_outcome(state, action)\n",
    "    return next_state, reward\n",
    "    \n",
    "def call_policy(state, value, params):\n",
    "    \"\"\"\n",
    "    Call a policy to choose actions, given current state and value function.\n",
    "    Args:\n",
    "        state: integer corresponding to the current state.\n",
    "        value: a matrix indexed by state and action.\n",
    "        params: a dictionary containing the default parameters.\n",
    "    Returns:\n",
    "        an integer corresponding action chosen according to the policy.\n",
    "    \"\"\"\n",
    "    # multiple options for policy\n",
    "    if params['policy'] == 'epsilon_greedy':\n",
    "        return epsilon_greedy(state, value, params)\n",
    "    elif params['policy'] == 'softmax':\n",
    "        return softmax(state, value, params)\n",
    "    else: # random policy (if policy not recognized, choose randomly)\n",
    "        return randint(params['environment'].n_actions)\n",
    "\n",
    "def update_value(prev_state, action, reward, state, value, params):\n",
    "    \"\"\"\n",
    "    Update the value function.\n",
    "    Args:\n",
    "        prev_state: an integer corresponding to the previous state.\n",
    "        action: an integer correspoding to action taken.\n",
    "        reward: a float corresponding to the reward received.\n",
    "        state: an integer corresponding to the current state;\n",
    "          should be None if the episode ended.\n",
    "        value: a matrix indexed by state and action.\n",
    "        params: a dictionary containing the default parameters. \n",
    "    Returns:\n",
    "        the updated value function (matrix indexed by state and action).\n",
    "    \"\"\"\n",
    "    if params['learning_rule'] == 'q_learning':\n",
    "        # off policy learning\n",
    "        return q_learning(prev_state, action, reward, state, value, params)\n",
    "    elif params['learning_rule'] == 'sarsa':\n",
    "        # on policy learning\n",
    "        return sarsa(prev_state, action, reward, state, value, params)\n",
    "    else:\n",
    "        print('Learning rule not recognized')\n",
    "\n",
    "def default_params(environment):\n",
    "    \"\"\"\n",
    "    Define the default parameters.\n",
    "    Args:\n",
    "        environment: an object corresponding to the environment.\n",
    "    Returns:\n",
    "        a dictionary containing the default parameters, where the keys\n",
    "            are strings (parameter names).\n",
    "    \"\"\"\n",
    "    params = dict()\n",
    "    params['environment'] = environment\n",
    "    \n",
    "    params['alpha'] = 0.1  # learning rate    \n",
    "    params['beta'] = 10  # inverse temperature    \n",
    "    params['policy'] = 'epsilon_greedy'\n",
    "    params['epsilon'] = 0.05  # epsilon-greedy policy\n",
    "    params['learning_rule'] = 'q_learning'\n",
    "    params['epsilon_decay'] = 0.9\n",
    "    \n",
    "    if environment.name == 'windy_cliff_grid':\n",
    "        params['gamma'] = 0.6  # temporal discount factor\n",
    "    elif environment.name == 'n_armed_bandit':\n",
    "        params['gamma'] = 0.9  # temporal discount factor\n",
    "    elif environment.name == 'cliff_world':\n",
    "        params['gamma'] = 1.0  # no discounting\n",
    "    elif environment.name == 'cheese_world':\n",
    "        params['gamma'] = 0.5  # temporal discount factor\n",
    "    elif environment.name == 'quentins_world':\n",
    "        params['gamma'] = 0.9  # temporal discount factor\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: $\\epsilon$ decay\n",
    "\n",
    "1. Modify the code you wrote for the $\\epsilon$-greedy policy so that the $\\epsilon$ (proportion of random choices) decreases over time.\n",
    "2. Test the new policy with different learning algorithms. How does the decay affect your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Sarsa($\\lambda$)\n",
    "\n",
    "1. You can improve the performance of TD algorithms by allowing faster back propagation. Modify your Sarsa algorithm from the previous tutorial to obtain Sarsa($\\lambda$), which makes use of eligibility traces to propagate the strengthening effect of each reward into the whole sequence of actions that led to it.\n",
    "2. To explore the effect of eligibility traces on learning, run Sarsa($\\lambda$) on Quentin's world, testing different combinations of $\\lambda$ and $\\alpha$ values. How does $\\lambda$ affect the efficiency of Sarsa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
