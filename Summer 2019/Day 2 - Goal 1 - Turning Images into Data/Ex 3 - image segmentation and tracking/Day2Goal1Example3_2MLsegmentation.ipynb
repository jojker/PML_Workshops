{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the previous notebook, we have used some functions with machine learning methods under the hood. But that is definitely not enough for this workshop. Given the limited time and number of topics we would like to cover, we can not talk too much in detail about the structures of all the nrural networks used for this task. If you are interested in them, [this blog](https://towardsdatascience.com/semantic-segmentation-with-deep-learning-a-guide-and-code-e52fc8958823) provides a very fine waklthrough of these networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation: tf.keras\n",
    "\n",
    "workflow:\n",
    "1. Visualize data/perform some exploratory data analysis\n",
    "2. Set up data pipeline and preprocessing\n",
    "3. Build model\n",
    "4. Train model\n",
    "5. Evaluate model\n",
    "6. Repeat\n",
    "\n",
    "Time Estimated: 60 min\n",
    "\n",
    "Originally by: Raymond Yuan, Software Engineering Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualize data/perform some exploratory data analysis\n",
    "### Get all the files\n",
    "\n",
    "Since this tutorial will be using a dataset from Kaggle, it requires creating an API Token for your Kaggle account, and uploading it. You need to install the Kaggle package with the following command:\n",
    "\n",
    "```\n",
    "pip install kaggle\n",
    "```\n",
    "\n",
    "More details can be found on the [official documentation page](https://www.kaggle.com/docs/api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfcontrib\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Upload the API token.\n",
    "def get_kaggle_credentials():\n",
    "    token_dir = os.path.join(os.path.expanduser(\"~\"),\".kaggle\")\n",
    "    token_file = os.path.join(token_dir, \"kaggle.json\")\n",
    "    if not os.path.isdir(token_dir):\n",
    "        os.mkdir(token_dir)\n",
    "    try:\n",
    "        with open(token_file,'r') as f:\n",
    "        pass\n",
    "    except IOError as no_file:\n",
    "        try:\n",
    "            from google.colab import files\n",
    "    except ImportError:\n",
    "        raise no_file\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if \"kaggle.json\" not in uploaded:\n",
    "        raise ValueError(\"You need an API key! see: \"\n",
    "                       \"https://github.com/Kaggle/kaggle-api#api-credentials\")\n",
    "    with open(token_file, \"wb\") as f:\n",
    "        f.write(uploaded[\"kaggle.json\"])\n",
    "    os.chmod(token_file, 600)\n",
    "\n",
    "get_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important__: Only import kaggle after adding the credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_name = 'carvana-image-masking-challenge'\n",
    "# Download data from Kaggle and unzip the files of interest. \n",
    "def load_data_from_zip(competition, file):\n",
    "    with zipfile.ZipFile(os.path.join(competition, file), \"r\") as zip_ref:\n",
    "        unzipped_file = zip_ref.namelist()[0]\n",
    "        zip_ref.extractall(competition)\n",
    "\n",
    "def get_data(competition):\n",
    "    kaggle.api.competition_download_files(competition, competition)\n",
    "    load_data_from_zip(competition, 'train.zip')\n",
    "    load_data_from_zip(competition, 'train_masks.zip')\n",
    "    load_data_from_zip(competition, 'train_masks.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(competition_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must accept the competition rules before downloading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = os.path.join(competition_name, \"train\")\n",
    "label_dir = os.path.join(competition_name, \"train_masks\")\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(competition_name, 'train_masks.csv'))\n",
    "ids_train = df_train['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "\n",
    "x_train_filenames = []\n",
    "y_train_filenames = []\n",
    "for img_id in ids_train:\n",
    "    x_train_filenames.append(os.path.join(img_dir, \"{}.jpg\".format(img_id)))\n",
    "    y_train_filenames.append(os.path.join(label_dir, \"{}_mask.gif\".format(img_id)))\n",
    "\n",
    "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = \\\n",
    "                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=42)\n",
    "\n",
    "num_train_examples = len(x_train_filenames)\n",
    "num_val_examples = len(x_val_filenames)\n",
    "\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of validation examples: {}\".format(num_val_examples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_num = 5\n",
    "\n",
    "r_choices = np.random.choice(num_train_examples, display_num)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i in range(0, display_num * 2, 2):\n",
    "    img_num = r_choices[i // 2]\n",
    "    x_pathname = x_train_filenames[img_num]\n",
    "    y_pathname = y_train_filenames[img_num]\n",
    "  \n",
    "    plt.subplot(display_num, 2, i + 1)\n",
    "    plt.imshow(mpimg.imread(x_pathname))\n",
    "    plt.title(\"Original Image\")\n",
    "  \n",
    "    example_labels = Image.open(y_pathname)\n",
    "    label_vals = np.unique(example_labels)\n",
    "  \n",
    "    plt.subplot(display_num, 2, i + 2)\n",
    "    plt.imshow(example_labels)\n",
    "    plt.title(\"Masked Image\")  \n",
    "\n",
    "plt.suptitle(\"Examples of Images and their Masks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (256, 256, 3)\n",
    "batch_size = 3\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up data pipeline and preprocessing\n",
    "1. Read the bytes of the file in from the filename - for both the image and the label. Recall that our labels are actually images with each pixel annotated as car or background (1, 0).\n",
    "2. Decode the bytes into an image format\n",
    "3. Apply image transformations: (optional, according to input parameters)\n",
    "    - resize - Resize our images to a standard size (as determined by eda or computation/memory restrictions)\n",
    "        + The reason why this is optional is that U-Net is a fully convolutional network (e.g. with no fully connected units) and is thus not dependent on the input size. However, if you choose to not resize the images, you must use a batch size of 1, since you cannot batch variable image size together\n",
    "        + Alternatively, you could also bucket your images together and resize them per mini-batch to avoid resizing images as much, as resizing may affect your performance through interpolation, etc.\n",
    "    - hue_delta - Adjusts the hue of an RGB image by a random factor. This is only applied to the actual image (not our label image). The hue_delta must be in the interval [0, 0.5]\n",
    "    - horizontal_flip - flip the image horizontally along the central axis with a 0.5 probability. This transformation must be applied to both the label and the actual image.\n",
    "    - width_shift_range and height_shift_range are ranges (as a fraction of total width or height) within which to randomly translate the image either horizontally or vertically. This transformation must be applied to both the label and the actual image.\n",
    "    - rescale - rescale the image by a certain factor, e.g. 1/ 255.\n",
    "4. Shuffle the data, repeat the data (so we can iterate over it multiple times across epochs), batch the data, then prefetch a batch (for efficiency).\n",
    "### Processing each pathname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_pathnames(fname, label_path):\n",
    "    # We map this function onto each pathname pair  \n",
    "    img_str = tf.read_file(fname)\n",
    "    img = tf.image.decode_jpeg(img_str, channels=3)\n",
    "\n",
    "    label_img_str = tf.read_file(label_path)\n",
    "    # These are gif images so they return as (num_frames, h, w, c)\n",
    "    label_img = tf.image.decode_gif(label_img_str)[0]\n",
    "    # The label image should only have values of 1 or 0, indicating pixel wise\n",
    "    # object (car) or not (background). We take the first channel only. \n",
    "    label_img = label_img[:, :, 0]\n",
    "    label_img = tf.expand_dims(label_img, axis=-1)\n",
    "    return img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_img(output_img, label_img, width_shift_range, height_shift_range):\n",
    "    \"\"\"This fn will perform the horizontal or vertical shift\"\"\"\n",
    "    if width_shift_range or height_shift_range:\n",
    "        if width_shift_range:\n",
    "            width_shift_range = tf.random_uniform([], \n",
    "                                              -width_shift_range * img_shape[1],\n",
    "                                              width_shift_range * img_shape[1])\n",
    "        if height_shift_range:\n",
    "            height_shift_range = tf.random_uniform([],\n",
    "                                               -height_shift_range * img_shape[0],\n",
    "                                               height_shift_range * img_shape[0])\n",
    "        # Translate both \n",
    "        output_img = tfcontrib.image.translate(output_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "        label_img = tfcontrib.image.translate(label_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "    return output_img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping the image randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img(horizontal_flip, tr_img, label_img):\n",
    "    if horizontal_flip:\n",
    "        flip_prob = tf.random_uniform([], 0.0, 1.0)\n",
    "        tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n",
    "                                lambda: (tr_img, label_img))\n",
    "    return tr_img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling our transformations into our augment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _augment(img,\n",
    "             label_img,\n",
    "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
    "             scale=1,  # Scale image e.g. 1 / 255.\n",
    "             hue_delta=0,  # Adjust the hue of an RGB image by random factor\n",
    "             horizontal_flip=False,  # Random left right flip,\n",
    "             width_shift_range=0,  # Randomly translate the image horizontally\n",
    "             height_shift_range=0):  # Randomly translate the image vertically \n",
    "    if resize is not None:\n",
    "        # Resize both images\n",
    "        label_img = tf.image.resize_images(label_img, resize)\n",
    "        img = tf.image.resize_images(img, resize)\n",
    "  \n",
    "    if hue_delta:\n",
    "        img = tf.image.random_hue(img, hue_delta)\n",
    "  \n",
    "    img, label_img = flip_img(horizontal_flip, img, label_img)\n",
    "    img, label_img = shift_img(img, label_img, width_shift_range, height_shift_range)\n",
    "    label_img = tf.to_float(label_img) * scale\n",
    "    img = tf.to_float(img) * scale \n",
    "    return img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_dataset(filenames, \n",
    "                         labels,\n",
    "                         preproc_fn=functools.partial(_augment),\n",
    "                         threads=5, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True):           \n",
    "    num_x = len(filenames)\n",
    "    # Create a dataset from the filenames and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Map our preprocessing function to every element in our dataset, taking\n",
    "    # advantage of multithreading\n",
    "    dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "    if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
    "        assert batch_size == 1, \"Batching images must be of the same size\"\n",
    "\n",
    "    dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
    "  \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(num_x)\n",
    "  \n",
    "  \n",
    "    # It's necessary to repeat our data for all epochs \n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1 / 255.,\n",
    "    'hue_delta': 0.1,\n",
    "    'horizontal_flip': True,\n",
    "    'width_shift_range': 0.1,\n",
    "    'height_shift_range': 0.1\n",
    "}\n",
    "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1 / 255.,\n",
    "}\n",
    "val_preprocessing_fn = functools.partial(_augment, **val_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_baseline_dataset(x_train_filenames,\n",
    "                                y_train_filenames,\n",
    "                                preproc_fn=tr_preprocessing_fn,\n",
    "                                batch_size=batch_size)\n",
    "val_ds = get_baseline_dataset(x_val_filenames,\n",
    "                              y_val_filenames, \n",
    "                              preproc_fn=val_preprocessing_fn,\n",
    "                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ds = get_baseline_dataset(x_train_filenames, \n",
    "                               y_train_filenames,\n",
    "                               preproc_fn=tr_preprocessing_fn,\n",
    "                               batch_size=1,\n",
    "                               shuffle=False)\n",
    "# Let's examine some of these augmented images\n",
    "data_aug_iter = temp_ds.make_one_shot_iterator()\n",
    "next_element = data_aug_iter.get_next()\n",
    "with tf.Session() as sess: \n",
    "    batch_of_imgs, label = sess.run(next_element)\n",
    "\n",
    "    # Running next element in our graph will produce a batch of images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    img = batch_of_imgs[0]\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(label[0, :, :, 0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, num_filters):\n",
    "    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "    encoder = layers.BatchNormalization()(encoder)\n",
    "    encoder = layers.Activation('relu')(encoder)\n",
    "    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "    encoder = layers.BatchNormalization()(encoder)\n",
    "    encoder = layers.Activation('relu')(encoder)\n",
    "    return encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    encoder = conv_block(input_tensor, num_filters)\n",
    "    encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "    \n",
    "    return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "    decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "    decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "    decoder = layers.BatchNormalization()(decoder)\n",
    "    decoder = layers.Activation('relu')(decoder)\n",
    "    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "    decoder = layers.BatchNormalization()(decoder)\n",
    "    decoder = layers.Activation('relu')(decoder)\n",
    "    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "    decoder = layers.BatchNormalization()(decoder)\n",
    "    decoder = layers.Activation('relu')(decoder)\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=img_shape)\n",
    "# 256\n",
    "\n",
    "encoder0_pool, encoder0 = encoder_block(inputs, 32)\n",
    "# 128\n",
    "\n",
    "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n",
    "# 64\n",
    "\n",
    "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n",
    "# 32\n",
    "\n",
    "encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n",
    "# 16\n",
    "\n",
    "encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n",
    "# 8\n",
    "\n",
    "center = conv_block(encoder4_pool, 1024)\n",
    "# center\n",
    "\n",
    "decoder4 = decoder_block(center, encoder4, 512)\n",
    "# 16\n",
    "\n",
    "decoder3 = decoder_block(decoder4, encoder3, 256)\n",
    "# 32\n",
    "\n",
    "decoder2 = decoder_block(decoder3, encoder2, 128)\n",
    "# 64\n",
    "\n",
    "decoder1 = decoder_block(decoder2, encoder1, 64)\n",
    "# 128\n",
    "\n",
    "decoder0 = decoder_block(decoder1, encoder0, 32)\n",
    "# 256\n",
    "\n",
    "outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom metrics and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '/tmp/weights.hdf5'\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_dice_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify our model callback in the fit function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, \n",
    "                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
    "                   epochs=epochs,\n",
    "                   validation_data=val_ds,\n",
    "                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n",
    "                   callbacks=[cp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "### Visualize training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = history.history['dice_loss']\n",
    "val_dice = history.history['val_dice_loss']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, dice, label='Training Dice Loss')\n",
    "plt.plot(epochs_range, val_dice, label='Validation Dice Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Dice Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize actual performance\n",
    "To load our model we have two options:\n",
    "\n",
    "1. Since our model architecture is already in memory, we can simply call `load_weights(save_model_path)`\n",
    "2. If you wanted to load the model from scratch (in a different setting without already having the model architecture in memory) we simply call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, load the weights directly: model.load_weights(save_model_path)\n",
    "model = models.load_model(save_model_path, custom_objects={'bce_dice_loss': bce_dice_loss,\n",
    "                                                           'dice_loss': dice_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize some of the outputs \n",
    "data_aug_iter = val_ds.make_one_shot_iterator()\n",
    "next_element = data_aug_iter.get_next()\n",
    "\n",
    "# Running next element in our graph will produce a batch of images\n",
    "plt.figure(figsize=(10, 20))\n",
    "for i in range(5):\n",
    "    batch_of_imgs, label = tf.keras.backend.get_session().run(next_element)\n",
    "    img = batch_of_imgs[0]\n",
    "    predicted_label = model.predict(batch_of_imgs)[0]\n",
    "  \n",
    "    plt.subplot(5, 3, 3 * i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Input image\")\n",
    "    \n",
    "    plt.subplot(5, 3, 3 * i + 2)\n",
    "    plt.imshow(label[0, :, :, 0])\n",
    "    plt.title(\"Actual Mask\")\n",
    "    plt.subplot(5, 3, 3 * i + 3)\n",
    "    plt.imshow(predicted_label[:, :, 0])\n",
    "    plt.title(\"Predicted Mask\")\n",
    "plt.suptitle(\"Examples of Input Image, Label, and Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Home Message\n",
    "- __Functional API__ - we implemented UNet with the Functional API. Functional API gives a lego-like API that allows us to build pretty much any network.\n",
    "- __Custom Losses and Metrics__ - We implemented custom metrics that allow us to see exactly what we need during training time. In addition, we wrote a custom loss function that is specifically suited to our task.\n",
    "- __Save and load our model__ - We saved our best model that we encountered according to our specified metric. When we wanted to perform inference with out best model, we loaded it from disk. Note that saving the model capture more than just the weights of the model: by default, it saves the model architecture, weights, as well as information about the training process such as the state of the optimizer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation: PyTorch\n",
    "\n",
    "## Input and output\n",
    "\n",
    "The input dimension is [Ni x Ci x Hi x Wi]\n",
    "where,\n",
    "\n",
    "- Ni -> the batch size\n",
    "- Ci -> the number of channels (which is 3)\n",
    "- Hi -> the height of the image\n",
    "- Wi -> the width of the image\n",
    "\n",
    "And the output dimension of the model is [No x Co x Ho x Wo]\n",
    "where,\n",
    "\n",
    "- No -> is the batch size (same as Ni)\n",
    "- Co -> is the number of classes that the dataset have!\n",
    "- Ho -> the height of the image (which is the same as Hi in almost all cases)\n",
    "- Wo -> the width of the image (which is the same as Wi in almost all cases)\n",
    " \n",
    "## FCN - fully convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from torchvision import models\n",
    "fcn = models.segmentation.fcn_resnet101(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    " \n",
    "!wget -nv https://static.independent.co.uk/s3fs-public/thumbnails/image/2018/04/10/19/pinyon-jay-bird.jpg -O bird.png\n",
    "img = Image.open('./bird.png')\n",
    "plt.imshow(img); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the image, apply the transformations needed\n",
    "import torchvision.transforms as T\n",
    "trf = T.Compose([T.Resize(256),\n",
    "                 T.CenterCrop(224),\n",
    "                 T.ToTensor(), \n",
    "                 T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                             std = [0.229, 0.224, 0.225])])\n",
    "inp = trf(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input through the net\n",
    "out = fcn(inp)['out']\n",
    "print (out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode output\n",
    "# Define the helper function\n",
    "def decode_segmap(image, nc=21):\n",
    "   \n",
    "    label_colors = np.array([(0, 0, 0),  # 0=background\n",
    "                 # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "                 (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n",
    "                 # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "                 (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n",
    "                 # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "                 (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n",
    "                 # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "                 (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])\n",
    "   \n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "     \n",
    "    for l in range(0, nc):\n",
    "        idx = image == l\n",
    "        r[idx] = label_colors[l, 0]\n",
    "        g[idx] = label_colors[l, 1]\n",
    "        b[idx] = label_colors[l, 2]\n",
    "         \n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result\n",
    "def segment(net, path):\n",
    "    img = Image.open(path)\n",
    "    plt.imshow(img); plt.axis('off'); plt.show()\n",
    "    # Comment the Resize and CenterCrop for better inference results\n",
    "    trf = T.Compose([T.Resize(256), \n",
    "                     T.CenterCrop(224), \n",
    "                     T.ToTensor(), \n",
    "                     T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                                 std = [0.229, 0.224, 0.225])])\n",
    "    inp = trf(img).unsqueeze(0)\n",
    "    out = net(inp)['out']\n",
    "    om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
    "    rgb = decode_segmap(om)\n",
    "    plt.imshow(rgb); plt.axis('off'); plt.show()\n",
    "!wget -nv https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg -O horse.png\n",
    "segment(fcn, './horse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLab\n",
    "a Semantic Segmentation Architecture that came out of Google Brain. Letâ€™s see how we can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlab = models.segmentation.deeplabv3_resnet101(pretrained=1).eval()\n",
    "segment(dlab, './horse.png') # change the file name to your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple objects\n",
    "!wget -nv \"https://images.pexels.com/photos/2385051/pexels-photo-2385051.jpeg\" -O dog-park.png\n",
    "img = Image.open('./dog-park.png')\n",
    "plt.imshow(img); plt.show()\n",
    " \n",
    "print ('Segmenatation Image on FCN')\n",
    "segment(fcn, path='./dog-park.png', show_orig=False)\n",
    " \n",
    "print ('Segmenatation Image on DeepLabv3')\n",
    "segment(dlab, path='./dog-park.png', show_orig=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
