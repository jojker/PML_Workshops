{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLworkshop_ClusteringIntro-KMeansAndBeyond.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lCmC-sNpNk-S","colab_type":"text"},"source":["Clustering is one of the ways for unsupervised learning. It is useful when we want to see the internal groupings of an unlabeled data set. The **goal** of this notebook is to introduce the **basic clustering approaches** and their **implementations in Python**. \n","\n","\n","# K-means clustering\n"]},{"cell_type":"markdown","metadata":{"id":"sd-zyBUL_MIK","colab_type":"text"},"source":["## How does it work\n","\n","* Guess some cluster centers\n","* Repeat until converged\n"," - E-Step: assign points to the nearest cluster center\n"," - M-Step: set the cluster centers to the mean\n","\n","\n","<figure>\n"," <img src=\"https://miro.medium.com/max/960/1*KrcZK0xYgTa4qFrVr0fO2w.gif\" alt=\"K-means process\" />\n"," <figcaption>\n","  K-means clustering in progress <a href=\"https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\">The 5 Clustering Algorithms Data Scientists Need to Know</a>\n"," </figcaption>\n","</figure>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8Nn_J60vhfSy","colab_type":"text"},"source":["## Assumptions (it works when...?)\n","Clusters should be spherical, well separated, have similar volumes and similar number of points.\n","\n","\n","We will be working on a toy dataset of delivery fleet driver (from [Introduction to K-means Clustering](https://https://www.datascience.com/blog/k-means-clustering)). Let's load the data and take a look first (run the following code chunk):\n"]},{"cell_type":"code","metadata":{"id":"oQ1Ub4x2h4xX","colab_type":"code","colab":{}},"source":["# Import delivery fleet driver data\n","## Import pandas. Used here for file loading. \n","## This package is widely used for data manipulation and analysis.\n","import pandas as pd \n","\n","url = 'https://raw.githubusercontent.com/datascienceinc/learn-data-science/master/Introduction-to-K-means-Clustering/Data/data_1024.csv'\n","# The URL above can be replaced by any source of a raw csv-format dataset\n","\n","data = pd.read_csv(url, sep = \"\\t\") # Read from the URL source as file.\n","\n","print(data.shape) # Show the dimsension of data \n","data.head() # See the top 5 rows"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1HYSYdZAnXjS","colab_type":"text"},"source":["So our data set contains Driver_ID, Distance, and Speeding information for 4000 drivers -> a 4000 rows by 2 columns matrix. We can describe the drivers by their distance and speeding features:"]},{"cell_type":"code","metadata":{"id":"vXk8HnRxpY7W","colab_type":"code","colab":{}},"source":["# Import modules for plotting and plotting styling\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()  # for plot styling"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BnvzV4SWp1C_","colab_type":"code","colab":{}},"source":["# Plot Distance as x-axis and Speeding as y-axis for drivers\n","plt.scatter(data[\"Distance_Feature\"], data[\"Speeding_Feature\"])\n","plt.xlabel(\"Distance\")\n","plt.ylabel(\"Speeding\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDQkNxdurGVR","colab_type":"text"},"source":["With naked eye, we can see roughly two to four groups of drivers, separated by their Distance and Speeding. Let's see how good can K-means do."]},{"cell_type":"code","metadata":{"id":"sQZbogFNrxcW","colab_type":"code","colab":{}},"source":["# Import modules required for Kmeans calculation\n","import numpy as np # Used for managing arrays\n","from sklearn.cluster import KMeans "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"de_n7EE4sgbQ","colab_type":"text"},"source":["## Data reformatting \n","We used pandas to read our csv file in, and by-default, it is stored as a pandas dataframe"]},{"cell_type":"code","metadata":{"id":"bCIYEw-lsJMK","colab_type":"code","colab":{}},"source":["type(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MBvdoVAg5-c6","colab_type":"text"},"source":["However, this format is not what KMeans can take. We need to reformat to meet the input requirement."]},{"cell_type":"code","metadata":{"id":"cGdA_JIMsdQd","colab_type":"code","colab":{}},"source":["# Formatting data\n","## scikit learn kmeans requires the input as ndarray.\n","## transform the pandas dataframe into arrays\n","## extract values from the columns 1 - Distance, 2 - Speeding \n","X = data.iloc[:,1:3].values  \n","print(type(X))\n","X"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M7Hk-D8H6lJz","colab_type":"text"},"source":["## Run K-means: interpretations and optimizations\n","Now, we can run the data set as an ndarray. Notice, k-means requires that we provide the number of clusters. From what we see, we can try two first."]},{"cell_type":"code","metadata":{"id":"1jSX-vvl66yA","colab_type":"code","colab":{}},"source":["# Use reformatted data X for K-means calculation\n","kmeans = KMeans(n_clusters=2).fit(X)\n","# Clustering assignments can be retrieved from .label_ .\n","# apply this assignment to the Speeding-by-Distance plot\n","plt.scatter(data[\"Distance_Feature\"], data[\"Speeding_Feature\"], \n","            c=kmeans.labels_, cmap='viridis')\n","# Plot the cluster centers on top\n","centers = kmeans.cluster_centers_\n","plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5) \n","plt.xlabel(\"Distance\")\n","plt.ylabel(\"Speeding\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ppK_jch6taXw","colab_type":"text"},"source":["Looks like the data grouped well with `n_clusters=2`. But we do see separations within each clusters. What would the optimal number for clustering this dataset and how would we know?\n","\n","Run the code chunk below, and see if you can find the number that works the best for this dataset (play around a couple of values for number of clusters).\n"]},{"cell_type":"code","metadata":{"id":"UVm_SXPhAVHP","colab_type":"code","colab":{}},"source":["# Test different numbers of k for the k-means result\n","# Import interactive modules \n","from ipywidgets import interact\n","import ipywidgets as widgets\n","# Define function to run interactively. Here we allow the number of clusters \n","# to be an interactive element in the KMeans calls.\n","def KMeans_test_n_cl(num_clusters):\n","    kmeans = KMeans(n_clusters=num_clusters).fit(X)\n","    # Clustering assignments can be retrieved from .label_ .\n","    # apply this assignment to the Speeding-by-Distance plot\n","    plt.scatter(data[\"Distance_Feature\"], data[\"Speeding_Feature\"], \n","                c=kmeans.labels_, cmap='viridis')\n","    # Plot the cluster centers on top\n","    centers = kmeans.cluster_centers_\n","    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5) \n","    plt.xlabel(\"Distance\")\n","    plt.ylabel(\"Speeding\")\n","    plt.show()\n","    return\n","# Run interact\n","interact(KMeans_test_n_cl, \n","         num_clusters=widgets.IntSlider(min=2, max=10, step=1, value=2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X7GhQEYAAWgt","colab_type":"text"},"source":["Testing manually could be annoying when dealing with more complex datasets. One automatic optimization approach would be the **silhouette analysis**. Ranging from [-1,1], the silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). Higher value indicates that the object is well matched to its own cluster. If most objects have a high value, then the clustering configuration is appropriate ([wiki/Silhouette_(clustering)](https://https://en.wikipedia.org/wiki/Silhouette_(clustering)))."]},{"cell_type":"code","metadata":{"id":"4uox1EGFFTRE","colab_type":"code","colab":{}},"source":["# Import modules for silhouette calculations\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","import matplotlib.cm as cm\n","\n","# We'll just test within the range of [2,6]. \n","# Any range you are interested in testing could replace this\n","for n_clusters in range(2,6):\n","    # Create a subplot with 1 row and 2 columns\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","    fig.set_size_inches(18, 7)\n","\n","    # The 1st subplot is the silhouette plot\n","    # The silhouette coefficient can range from -1, 1 but in this example all\n","    # lie within [-0.1, 1]\n","    ax1.set_xlim([-0.1, 1])\n","    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n","    # plots of individual clusters, to demarcate them clearly.\n","    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n","    # Initialize the clusterer with n_clusters value and a random generator\n","    # seed of 10 for reproducibility.\n","    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n","    kmeans = clusterer.fit(X)\n","    cluster_labels = kmeans.labels_\n","    \n","    # The silhouette_score gives the average value for all the samples.\n","    # This gives a perspective into the density and separation of the formed\n","    # clusters \n","    silhouette_avg = silhouette_score(X, cluster_labels)\n","    print(\"For n_clusters =\", n_clusters,\n","          \"The average silhouette_score is :\", silhouette_avg)\n","\n","\n","    # Compute the silhouette scores for each sample\n","    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n","\n","    y_lower = 10\n","    for i in range(n_clusters):\n","        # Aggregate the silhouette scores for samples belonging to\n","        # cluster i, and sort them\n","        ith_cluster_silhouette_values = \\\n","            sample_silhouette_values[cluster_labels == i]\n","\n","        ith_cluster_silhouette_values.sort()\n","\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","\n","        color = cm.nipy_spectral(float(i) / n_clusters)\n","        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n","                          0, ith_cluster_silhouette_values,\n","                          facecolor=color, edgecolor=color, alpha=0.7)\n","\n","        # Label the silhouette plots with their cluster numbers at the middle\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","        # Compute the new y_lower for next plot\n","        y_lower = y_upper + 10  # 10 for the 0 samples\n","\n","    ax1.set_title(\"The silhouette plot for the various clusters.\")\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","\n","    # The vertical line for average silhouette score of all the values\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","\n","    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","    \n","    # 2nd Plot showing the actual clusters formed\n","    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n","    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n","                c=colors, edgecolor='k')\n","\n","    # Labeling the clusters\n","    centers = clusterer.cluster_centers_\n","    # Draw white circles at cluster centers\n","    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n","                c=\"white\", alpha=1, s=200, edgecolor='k')\n","\n","    for i, c in enumerate(centers):\n","        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n","                    s=50, edgecolor='k')\n","\n","    ax2.set_title(\"The visualization of the clustered data.\")\n","    ax2.set_xlabel(\"Feature space for the 1st feature\")\n","    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n","\n","    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n","                  \"with n_clusters = %d\" % n_clusters),\n","                 fontsize=14, fontweight='bold')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZqyyNKpXJ65K","colab_type":"text"},"source":["A good number of clusters should meet the following:\n","1. The mean value should be as close to 1 as possible\n","2. The plot of each cluster should be above the mean value as much as possible. Any plot region below the mean value is not desirable.\n","3. The width of the plot should be as uniform as possible.\n","\n","(from [USING SILHOUETTE ANALYSIS FOR SELECTING THE NUMBER OF CLUSTER FOR K-MEANS CLUSTERING](http://kapilddatascience.wordpress.com/2015/11/10/using-silhouette-analysis-for-selecting-the-number-of-cluster-for-k-means-clustering/))\n","\n","Although four-cluster assignment does look better, two-cluster assignment got the highest score. This could be due the fact that the data set itself does not fit the assumptions that well. "]},{"cell_type":"code","metadata":{"id":"OnV-BJLNK0-k","colab_type":"code","colab":{}},"source":["# If only interested in the average silhouette score: \n","for n_clusters in range(2,6):\n","  # Initialize the clusterer with n_clusters value and a random generator\n","    # seed of 10 for reproducibility.\n","    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n","    kmeans = clusterer.fit(X)\n","    cluster_labels = kmeans.labels_\n","    \n","    # The silhouette_score gives the average value for all the samples.\n","    # This gives a perspective into the density and separation of the formed\n","    # clusters \n","    silhouette_avg = silhouette_score(X, cluster_labels)\n","    print(\"For n_clusters =\", n_clusters,\n","          \"The average silhouette_score is :\", silhouette_avg)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bG-ynuAiKyGJ","colab_type":"text"},"source":["## Ending notes for K-means\n","People love k-means for its speedity and simplicity (when the sample number is small). But there are a couple of things not so great for k-means:\n","1. it can get stuck in local minimals due to bias in initiation\n","2. users need to make the decision of how many clusters should in the data\n","3. it assumes clusters to be spherical, well-separated, similar in volume and the number of samples they contain\n","\n","To avoid (1), it is often recommended to run the calculation multiple times (scikit learn is doing it by default, so no worries :tada:). As for (2), we can run silhouette analysis for estimation. \n","\n","But if you are dealing with (3), there seem to be no workarounds to make k-means to give a better result. One quick alternative to try is the Gaussian mixture models (GMM)."]},{"cell_type":"markdown","metadata":{"id":"2qjAEc4F-DPy","colab_type":"text"},"source":["# Gaussian mixture model\n","Every sample in k-means belongs and only belongs to one cluster, which means there is no room for uncertainty. That's why k-means is also called a \"hard\" clustering method. What if we take a softer approach?"]},{"cell_type":"markdown","metadata":{"id":"KBKJSXqP_Ub7","colab_type":"text"},"source":["## Generalizing K-means\n","A Gaussian mixture model (GMM) is a **probabilistic model** that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. In a way, GMM is generaling the k-means model.\n","\n","<figure>\n"," <img src=\"https://miro.medium.com/max/1400/1*lTv7e4Cdlp738X_WFZyZHA.png\" alt=\"Gaussian mixture model\" width=\"500\"/>\n"," <figcaption>\n","  Graphical illustration of Gaussian mixture model from <a href=\"https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95\">Gaussian Mixture Models Explained</a>\n"," </figcaption>\n","</figure>\n","\n","<figure>\n"," <img src=\"https://miro.medium.com/max/1750/1*I0WTzTOyyDVwfPyMSZPzWQ.gif\" alt=\"GMM in progress\" width=\"500\"/>\n"," <figcaption>\n","  GMM clustering in progress from <a href=\"https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95\">Gaussian Mixture Models Explained</a>\n"," </figcaption>\n","</figure>\n","\n","\n","Let's see if this can work better than k-means for our data.\n"]},{"cell_type":"markdown","metadata":{"id":"iXlQg8q7-0Ph","colab_type":"text"},"source":["## Run GMM: interpretations and optimizations"]},{"cell_type":"code","metadata":{"id":"pXUs1e5b-pqT","colab_type":"code","colab":{}},"source":["# Import GMM module\n","from sklearn.mixture import GaussianMixture\n","# Specify number of clusters/components, and fit the model to data\n","# the same reformatted data X is used\n","gmm = GaussianMixture(n_components=4).fit(X) \n","# obtain labels for X\n","labels = gmm.predict(X)\n","# color samples with the cluster labels\n","plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7kOJIAsIWT4","colab_type":"text"},"source":["Since GMM uses probabilistic models, we can also visualize the clustering assignment \"strength\" of each sample. Here we'll show higher probability as large size."]},{"cell_type":"code","metadata":{"id":"4Qs5gCBmIVK7","colab_type":"code","colab":{}},"source":["# obtain the probability matrix from GMM\n","probs = gmm.predict_proba(X)\n","# convert probability into a size feature\n","size = 50 * probs.max(1) ** 2  # square emphasizes differences\n","# coloring dots by clustering assignment and tuning their size based on the\n","# probability of their belonging to the cluster\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxcooAFlCaET","colab_type":"text"},"source":["Note that GMM also requires a **prior estimation of number of clusters** (`n_components`), which leads to the same question of how would we know if this is correct? For GMM, we can use the criteria of Akaike information criterion (**AIC**) or the Bayesian information criterion (**BIC**). These are both included as attributes in the scikit learn GMM calculations."]},{"cell_type":"code","metadata":{"id":"liiRCx9JKcmX","colab_type":"code","colab":{}},"source":["# Calculate AIC and BIC for number of clusters from 1 to 10\n","n_components = np.arange(1, 11)\n","# create a list of models each use a different cluster number\n","models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X)\n","          for n in n_components]\n","# plot AIC and BIC trends versus number of clusters\n","plt.plot(n_components, [m.bic(X) for m in models], label='AIC')\n","plt.plot(n_components, [m.aic(X) for m in models], label='BIC')\n","plt.legend(loc='best')\n","plt.xlabel('n_components');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"04PuYsDx-OLN","colab_type":"text"},"source":["The optimal number of clusters is the one that **minimizes AIC or BIC value**. This time, four-component clustering does give us the best performance. Running an AIC or BIC estimation will allow you to pick the right number of clusters/components to use in the GMM calculation. This estimation itself can also be handled automatically if you use  **Bayesian Gaussian mixture model**, which implemented variational inference to infer the effective number of components from the data."]},{"cell_type":"code","metadata":{"id":"KpHsWb_qN6P4","colab_type":"code","colab":{}},"source":["# Import Bayesian Gaussian mixture model\n","from sklearn.mixture import BayesianGaussianMixture\n","# the same reformatted data X is used\n","bgmm = BayesianGaussianMixture(n_components=5, n_init=10).fit(X) \n","# obtain labels for X\n","labels = bgmm.predict(X)\n","# show number of clusters in the assignment\n","print(\"Number of clusters: %2d\" %(len(np.unique(labels))))\n","# color samples with the cluster labels\n","plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7aW2e_LVZWw","colab_type":"text"},"source":["We gave Bayesian GMM `n_components=5`. Instead of using all to fit the data, as what GMM would do, the Bayesian model effectively uses as many as are needed for a good fit (when the end result converges). As a result, we got the four-cluster assignment as in the optimized GMM.\n","\n","## Density estimations\n","\n","So far, what we've encountered have all required us to have prior knowledge about number of clusters. At the same time, they do not perform well when the shape of the clusters largely deviate from an ellipse (e.g., crescent moon). Let's see how GMM perform on this type of data (example from [In Depth: Gaussian Mixture Model](http://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)):\n"]},{"cell_type":"code","metadata":{"id":"ktSp6Bxr7bJ9","colab_type":"code","colab":{}},"source":["# Let's make some moons\n","from sklearn.datasets import make_moons\n","Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\n","plt.scatter(Xmoon[:, 0], Xmoon[:, 1]);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdHnemOS893C","colab_type":"code","colab":{}},"source":["# Define functions to show us the GMM result more intuitively\n","from matplotlib.patches import Ellipse\n","\n","def draw_ellipse(position, covariance, ax=None, **kwargs):\n","    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n","    ax = ax or plt.gca()\n","    \n","    # Convert covariance to principal axes\n","    if covariance.shape == (2, 2):\n","        U, s, Vt = np.linalg.svd(covariance)\n","        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n","        width, height = 2 * np.sqrt(s)\n","    else:\n","        angle = 0\n","        width, height = 2 * np.sqrt(covariance)\n","    \n","    # Draw the Ellipse\n","    for nsig in range(1, 4):\n","        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n","                             angle, **kwargs))\n","        \n","def plot_gmm(gmm, X, label=True, ax=None):\n","    ax = ax or plt.gca()\n","    labels = gmm.fit(X).predict(X)\n","    if label:\n","        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n","    else:\n","        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n","    ax.axis('equal')\n","    \n","    w_factor = 0.2 / gmm.weights_.max()\n","    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n","        draw_ellipse(pos, covar, alpha=w * w_factor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"74byzbUw7lwj","colab_type":"code","colab":{}},"source":["# Fit with 2-component GMM \n","gmm2 = GaussianMixture(n_components=2, covariance_type='full', random_state=0)\n","plot_gmm(gmm2, Xmoon)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZ7ey1yt9li-","colab_type":"text"},"source":["Although we have a clear estimate of total number of clusters should be there in the data, the returned result from GMM is not very useful. But if we instead use many more components and ignore the cluster labels, we find a fit that is much closer to the input data: "]},{"cell_type":"code","metadata":{"id":"8SY04HJm-Jul","colab_type":"code","colab":{}},"source":["# Fit with 16-component GMM\n","gmm16 = GaussianMixture(n_components=16, covariance_type='full', random_state=0)\n","plot_gmm(gmm16, Xmoon, label=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYFsi0HP-om5","colab_type":"text"},"source":["Here the mixture of 16 Gaussians serves not to find separated clusters of data, but rather to model the overall distribution of the input data. The GMM algorithm accomplishes this by representing the density as a weighted sum of Gaussian distributions. What if we push to the extreme and use the density feature of data to facilitate clustering?"]},{"cell_type":"markdown","metadata":{"id":"baUeAgRk-Tdz","colab_type":"text"},"source":["# DBSCAN\n","DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise, which is one of the ways taking advantage of density in data for clustering. It shares a lot in its logic with [Kernel density estimation (KDE)](https://jakevdp.github.io/PythonDataScienceHandbook/05.13-kernel-density-estimation.html), but uses different appraoch in the density estimation, thus less expensive to compute. We will only cover DBSCAN in this notebook, but feel free to also check out the link embedded above for detailed explanations for the general idea of KDE. "]},{"cell_type":"markdown","metadata":{"id":"jwQYa-b93cWi","colab_type":"text"},"source":["<figure>\n"," <img src=\"https://miro.medium.com/max/1688/1*tc8UF-h0nQqUfLC8-0uInQ.gif\" alt=\"DBSCAN on smiley face\" width=\"500\"/>\n"," <figcaption>\n","  DBSCAN smiley face clustering in progress (source from <a href=\"https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\">The 5 Clustering Algorithms Data Scientists Need to Know</a>). For clustering in real time, check out <a href=\"https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/?source=post_page---------------------------\">Visualizing DBSCAN Clustering</a>. \n"," </figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"Agmq_aqJCFqa","colab_type":"text"},"source":["Great things about **DBSCAN** include:\n","1. no requirement for a pre-set number of clusters\n","2. can discover clusters of arbitrary shapes\n","3. efficient for large data set (over a couple thousand)\n","\n","## How does it work?\n","\n","1. Arbitrary initiation, then retrieve all neighborhood information within distance ϵ.\n","2. If this point contains MinPts within ϵ neighborhood, cluster formation starts. Otherwise the point is labeled as noise. This point can be later found within the ϵ neighborhood of a different point and, thus can be made a part of the cluster.\n","3. points within the neighborhood of the first point become part of the cluster. So all the points found within ϵ neighborhood are added, along with their own ϵ neighborhood, if they themselves also contains MinPts within their ϵ neighborhood.\n","4. The above process continues until the density-connected cluster is completely found.\n","5. The process restarts with a new point which can be a part of a new cluster or labeled as noise.\n","\n"]},{"cell_type":"code","metadata":{"id":"nvhmzX2MHzaK","colab_type":"code","colab":{}},"source":["# Import DBSCAN module\n","from sklearn.cluster import DBSCAN\n","from sklearn.preprocessing import StandardScaler"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzEbgQxF_kym","colab_type":"text"},"source":["## Run DBSCAN: interpretations and optimizations"]},{"cell_type":"code","metadata":{"id":"R4VasgugW13F","colab_type":"code","colab":{}},"source":["# code credit from \n","# https://medium.com/@elutins/dbscan-what-is-it-when-to-use-it-how-to-use-it-8bd506293818\n","# Scale the data first\n","scaledX = StandardScaler().fit_transform(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OjiuirLMAM3q","colab_type":"text"},"source":["DBSCAN actually still require some prior knowledge of the data to set the two parameters: distance ϵ and threshold for number of points (minPts). Let's see how these two could affect the result:"]},{"cell_type":"code","metadata":{"id":"PBC-gpAVDDwJ","colab_type":"code","colab":{}},"source":["def DBSCAN_test_eps_minpts(eps=0.1, minPts = 1):\n","  # Setting up DBSCAN\n","  dbscan = DBSCAN(eps = eps, min_samples=minPts)\n","  # Fitting model\n","  model = dbscan.fit(scaledX)\n","  labels = model.labels_\n","\n","  from sklearn import metrics\n","  # identifying the core samples\n","  core_samples = np.zeros_like(labels, dtype=bool)\n","\n","  # core_samples[dbscan.core_sample_indices_] = True\n","  # print(core_samples)\n","\n","  # declare the number of clusters\n","  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n","  print(\"Number of clusters found: %d\" % n_clusters_)\n","\n","  # computing the Silouette Score\n","  print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(scaledX, labels))\n","  \n","  # Plot \n","  color_palette = sns.color_palette('Paired', 10)\n","  cluster_colors = [color_palette[x] if x >= 0\n","                  else (0.5, 0.5, 0.5)\n","                  for x in model.labels_]\n","  plt.scatter(scaledX[:,0],scaledX[:,1], c=cluster_colors, alpha=0.3)\n","  plt.show()\n","  return\n","\n","interact(DBSCAN_test_eps_minpts,\n","        eps = (0.1,1.0), \n","        minPts = widgets.IntSlider(min=1,max=10, step=1, value=10))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zb3Ujne1X0nA","colab_type":"text"},"source":["We again use the Silhouette score to measure the clustering performance. Gray dots in this plot are noises defined by DBSCAN. If we follow the recommended condition `minPts = 4`, we can see that there seem to be no right value for epsilon for a nice clustering result. As epsilon gets smaller, we start to see DBSCAN calling numerous random clusters from the noisy upper right corner.  \n","\n","This unsatisfactory performance goes back to the question of whether our data meets the assumptions made by the algorithm. DBSCAN is resistant to noise and can handle clusters that come in different shape and sizes. However, when clusters have varying densities, which is what our data looks, it does not work well."]},{"cell_type":"markdown","metadata":{"id":"yUHrxyuCsTeL","colab_type":"text"},"source":["\n","\n","\n",">  Side note: Finding the best values for minPts and epsilon?\n",">  - minPts\n","1. For two-dimensional data: use default value of minPts=4 (Ester et al., 1996)\n","2. For more than 2 dimensions: minPts=2*dim (Sander et al., 1998)\n",">\n","> - Estimate epsilon based on minPts\n"," 1. Plot the k-distances with k=minPts (Ester et al., 1996)\n"," 2. Find the 'elbow' in the graph--> The k-distance value is the Epsilon value."]},{"cell_type":"markdown","metadata":{"id":"LvtVR4Pk3Zf6","colab_type":"text"},"source":["Is there a way that allows us to use density estimation for clustering without being affected by the density variation across clusters? Methods including OPTICS and HDBSCAN are all worth considering. I provide ways to implement these methods below for you to try. \n","\n","Spoiler: although these method claims to do better than DBSCAN in when clusters vary in densities, they might not give better results if the parameters are not optimized."]},{"cell_type":"markdown","metadata":{"id":"zIvg9Ndw7ds6","colab_type":"text"},"source":["## OPTICS"]},{"cell_type":"markdown","metadata":{"id":"JGnIG85-Qhpc","colab_type":"text"},"source":["OPTICS stands for Ordering Points To Identify the Clustering Structure, a close relative of DBSCAN. It finds high density sample cores, and expands from there to form clusters. Cluster extraction step could still be done with the DBSCAN approach or more automatically by the \"xi\" approach [sklearn OPTICS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html).\n","OPTICS does a better job in keeping cluster hierarchy for a variable neighborhood radius, thus handles clusters with varying densities better."]},{"cell_type":"code","metadata":{"id":"A8tuGc8AQhZU","colab_type":"code","colab":{}},"source":["from sklearn.cluster import OPTICS\n","clust = OPTICS(min_samples=4, cluster_method='dbscan', eps = 0.2) # \n","\n","# Run the fit\n","labels = clust.fit_predict(scaledX)\n","color_palette = sns.color_palette('Paired', 10)\n","cluster_colors = [color_palette[x] if x >= 0\n","                  else (0.5, 0.5, 0.5)\n","                  for x in labels]\n","plt.scatter(scaledX[:,0],scaledX[:,1], c=cluster_colors, alpha=0.3)\n","n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n","print(\"Number of clusters: %d\" % (n_clusters))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0BBlBAIVZzNE","colab_type":"text"},"source":["## HDBSCAN\n"]},{"cell_type":"markdown","metadata":{"id":"F8Fr8N6olgTq","colab_type":"text"},"source":["HDBSCAN is an extended version of DBSCAN, converting it into  a hierarchical clustering algorithm, and then using a technique to extract a flat clustering based in the stability of clusters. See [Using HDBSCAN](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html)."]},{"cell_type":"code","metadata":{"id":"QoeAFhDUPCC-","colab_type":"code","colab":{}},"source":["!pip install hdbscan\n","import hdbscan"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOVKbFeDaEPs","colab_type":"code","colab":{}},"source":["clust = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=4)\n","\n","# Run the fit\n","clust.fit(scaledX)\n","labels = clust.labels_\n","\n","# Ignore noises in counting clusters\n","n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n","print(\"Number of clusters: %d\" % (n_clusters))\n","\n","# Set colors for plotting clusters\n","color_palette = sns.color_palette('Paired', 10)\n","cluster_colors = [color_palette[x] if x >= 0\n","                  else (0.5, 0.5, 0.5)\n","                  for x in labels]\n","plt.scatter(scaledX[:,0],scaledX[:,1], c=cluster_colors, alpha=0.25)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pJi95iwDXQsS"},"source":["# Take-home messages\n","\n","1. Know your data. Different density, shape, noise-level of the data could all affect the performance of clustering approaches.\n","2. Tune parameters for the better result. Default setting often times are not the setting to use.\n","3. Open to alternative clustering approaches.\n","\n","Hungry for more? Check out [here](https://scikit-learn.org/stable/modules/clustering.html) for a nice summary of clustering approaches by scikit learn. ***Happy clustering!***\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AhXKNaLGVYEc","colab_type":"text"},"source":["References:\n","\n","1. https://courses.cs.washington.edu/courses/cse446/15sp/slides/cdr.pdf\n","2. [The 5 Clustering Algorithms Data Scientists Need to Know](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n","3. [Introduction to K-means Clustering](https://https://www.datascience.com/blog/k-means-clustering)\n","4. [wiki/Silhouette_(clustering)](https://https://en.wikipedia.org/wiki/Silhouette_(clustering))\n","5.  [USING SILHOUETTE ANALYSIS FOR SELECTING THE NUMBER OF CLUSTER FOR K-MEANS CLUSTERING](http://kapilddatascience.wordpress.com/2015/11/10/using-silhouette-analysis-for-selecting-the-number-of-cluster-for-k-means-clustering/)\n","6. [Gaussian Mixture Models Explained](https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95)\n","7. [In Depth: Gaussian Mixture Model](http://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)\n","8. [Kernel density estimation (KDE)](https://jakevdp.github.io/PythonDataScienceHandbook/05.13-kernel-density-estimation.html)\n","9. [Visualizing DBSCAN Clustering](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/?source=post_page---------------------------)\n","\n"]}]}